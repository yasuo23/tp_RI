{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\belou\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import math\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_word_positions(text, word, tokenizer,stemmer,without):\n",
    "    # Initialize the stemmer\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer\n",
    "    if without==False:\n",
    "     stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "     stemmed_word =word\n",
    "     positions = []\n",
    "     for i, token in enumerate(tokens):\n",
    "        if stemmed_tokens[i] == stemmed_word:\n",
    "            positions.append(i+1)  # Append word index instead of character position\n",
    "\n",
    "     return positions\n",
    "    else:\n",
    "        positions = []\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token == word:\n",
    "             positions.append(i+1)  # Append word index instead of character position\n",
    "\n",
    "        return positions\n",
    "        \n",
    "    \n",
    "\n",
    "    # Find positions by word index\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitLancaste(text,i):\n",
    "    # Tokenize the text (split into words)\n",
    "    words = text.split()\n",
    "\n",
    "    # Get the list of stopwords in English\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    # Initialize the Porter Stemmer\n",
    "    porter = nltk.LancasterStemmer()\n",
    "    \n",
    "    normalized_terms =  set(porter.stem(word) for word in words if word.lower() not in stopwords)\n",
    "\n",
    "    TermesFrequence = FreqDist(normalized_terms)\n",
    "\n",
    "    max_freq = max(TermesFrequence.values())\n",
    "\n",
    "    return [(i, terme, freq, max_freq,find_word_positions(text, terme ,words,porter,False)) for terme, freq in TermesFrequence.items()]\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "\n",
    "\n",
    "def RegExPorter(text, i):\n",
    "    # Tokenize the text (split into words)\n",
    "    ExpReg = nltk.RegexpTokenizer(\n",
    "        '(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,\\-]\\d+)?%?|\\w+(?:[\\-/]\\w+)*')\n",
    "    words = ExpReg.tokenize(text)\n",
    "    \n",
    "    # Define stopwords and initialize the Porter Stemmer\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    porter = nltk.PorterStemmer()\n",
    "\n",
    "    # Create a dictionary to store positions of stemmed words\n",
    "\n",
    "    # Iterate over words, stem them, and find positions\n",
    "    # FreqDist for the normalized (stemmed) terms\n",
    "    normalized_terms = [porter.stem(word) for word in words if word.lower() not in stopwords]\n",
    "    TermesFrequence = FreqDist(normalized_terms)\n",
    "    max_freq = max(TermesFrequence.values())\n",
    "    # Prepare the result\n",
    "    return [(i, term, freq, max_freq,find_word_positions(text, term ,words,porter,False)) for term, freq in TermesFrequence.items()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegExLancaste(text,i):\n",
    "    # Tokenize the text \n",
    "    ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,\\-]\\d+)?%?|\\w+(?:[\\-/]\\w+)*') # \\d : équivalent à [0-9] \n",
    "    words = ExpReg.tokenize(text) \n",
    "    # Get the list of stopwords in English\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    # Initialize the Porter Stemmer\n",
    "    porter = nltk.LancasterStemmer()\n",
    "    \n",
    "    normalized_terms =  [porter.stem(word.lower())  for word in words if word.lower() not in stopwords]\n",
    "    \n",
    "    TermesFrequence = FreqDist(normalized_terms)\n",
    "\n",
    "    max_freq = max(TermesFrequence.values())\n",
    "    return [(i, terme, freq, max_freq,find_word_positions(text, terme ,words,porter,False)) for terme, freq in TermesFrequence.items()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitPorter(text, doc_id):\n",
    "    words = text.split()\n",
    "\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    porter = nltk.PorterStemmer()\n",
    "\n",
    "    normalized_terms =  [porter.stem(word.lower()) for word in words if word.lower() not in stopwords]\n",
    "\n",
    "    TermesFrequence = FreqDist(normalized_terms)\n",
    "\n",
    "    max_freq = max(TermesFrequence.values())\n",
    "\n",
    "    return [(doc_id, terme, freq, max_freq,find_word_positions(text, terme ,words,porter,False)) for terme, freq in TermesFrequence.items()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitwithout(text, doc_id):\n",
    "    words = text.split()\n",
    "\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "  \n",
    "\n",
    "    normalized_terms =  [word.lower() for word in words if word.lower() not in stopwords]\n",
    "\n",
    "    TermesFrequence = FreqDist(normalized_terms)\n",
    "\n",
    "    max_freq = max(TermesFrequence.values())\n",
    "\n",
    "    return [(doc_id, terme, freq, max_freq,find_word_positions(text, terme ,words,words,True)) for terme, freq in TermesFrequence.items()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegExwithout(text,i):\n",
    "    # Tokenize the text \n",
    "    ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,\\-]\\d+)?%?|\\w+(?:[\\-/]\\w+)*') # \\d : équivalent à [0-9] \n",
    "    words = ExpReg.tokenize(text) \n",
    "    # Get the list of stopwords in English\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    # Initialize the Porter Stemmer\n",
    "    \n",
    "    normalized_terms =  [word.lower() for word in words if word.lower() not in stopwords]\n",
    "    \n",
    "    TermesFrequence = FreqDist(normalized_terms)\n",
    "\n",
    "    max_freq = max(TermesFrequence.values())\n",
    "\n",
    "    return [(i, terme, freq, max_freq,find_word_positions(text, terme ,words,words,True)) for terme, freq in TermesFrequence.items()]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./test_dir/D_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      9\u001b[0m         file_content \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m---> 10\u001b[0m         processed_text \u001b[38;5;241m=\u001b[39m \u001b[43mj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[0;32m     11\u001b[0m         listw\u001b[38;5;241m.\u001b[39mextend(processed_text) \n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# N = 6 \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 15\u001b[0m, in \u001b[0;36msplitPorter\u001b[1;34m(text, doc_id)\u001b[0m\n\u001b[0;32m     11\u001b[0m TermesFrequence \u001b[38;5;241m=\u001b[39m FreqDist(normalized_terms)\n\u001b[0;32m     13\u001b[0m max_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(TermesFrequence\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [(doc_id, terme, freq, max_freq,find_word_positions(text, terme ,words,porter,\u001b[38;5;28;01mFalse\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m terme, freq \u001b[38;5;129;01min\u001b[39;00m TermesFrequence\u001b[38;5;241m.\u001b[39mitems()]\n",
      "Cell \u001b[1;32mIn[20], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m TermesFrequence \u001b[38;5;241m=\u001b[39m FreqDist(normalized_terms)\n\u001b[0;32m     13\u001b[0m max_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(TermesFrequence\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [(doc_id, terme, freq, max_freq,\u001b[43mfind_word_positions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterme\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43mporter\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m terme, freq \u001b[38;5;129;01min\u001b[39;00m TermesFrequence\u001b[38;5;241m.\u001b[39mitems()]\n",
      "Cell \u001b[1;32mIn[16], line 7\u001b[0m, in \u001b[0;36mfind_word_positions\u001b[1;34m(text, word, tokenizer, stemmer, without)\u001b[0m\n\u001b[0;32m      5\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m without\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m----> 7\u001b[0m  stemmed_tokens \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m      8\u001b[0m  stemmed_word \u001b[38;5;241m=\u001b[39mword\n\u001b[0;32m      9\u001b[0m  positions \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[16], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m without\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m----> 7\u001b[0m  stemmed_tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mstemmer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m      8\u001b[0m  stemmed_word \u001b[38;5;241m=\u001b[39mword\n\u001b[0;32m      9\u001b[0m  positions \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\belou\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\stem\\porter.py:662\u001b[0m, in \u001b[0;36mPorterStemmer.stem\u001b[1;34m(self, word, to_lowercase)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;124;03m:param to_lowercase: if `to_lowercase=True` the word always lowercase\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    660\u001b[0m stem \u001b[38;5;241m=\u001b[39m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mif\u001b[39;00m to_lowercase \u001b[38;5;28;01melse\u001b[39;00m word\n\u001b[1;32m--> 662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNLTK_EXTENSIONS \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m:\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool[stem]\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mORIGINAL_ALGORITHM \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    666\u001b[0m     \u001b[38;5;66;03m# With this line, strings of length 1 or 2 don't go through\u001b[39;00m\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;66;03m# the stemming process, although no mention is made of this\u001b[39;00m\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;66;03m# in the published algorithm.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "functions = [splitPorter,RegExPorter,RegExLancaste,splitLancaste,RegExwithout,splitwithout]\n",
    "# functions = [RegExPorter]\n",
    "\n",
    "listw = []\n",
    "\n",
    "for j in functions:\n",
    "    for i in range(1033):\n",
    "        with open(f'./test_dir/D_{i+1}.txt', 'r') as file:\n",
    "            file_content = file.read()\n",
    "            processed_text = j(file_content, i+1)  \n",
    "            listw.extend(processed_text) \n",
    "\n",
    "    # N = 6 \n",
    "    N=1033\n",
    "\n",
    "    weights = {}\n",
    "    for doc_id, term, freq, max_freq ,pos in listw:\n",
    "       ni=0\n",
    "       for d_id, t, f, m,p in listw:\n",
    "         if  t == term:\n",
    "           print(d_id)\n",
    "           ni+=1\n",
    "            \n",
    "      #  ni = sum(1 for d_id, t, f, m,p in listw if t == term)\n",
    "       weight = (freq / max_freq) * math.log10((N / (ni ))+1)\n",
    "       \n",
    "       print(ni)\n",
    "       print(N)\n",
    "       print (freq)\n",
    "       print(max_freq)\n",
    "       print( weight)\n",
    "       weights[(doc_id, term)] = (freq, weight, pos)\n",
    "\n",
    "    for (doc_id, term), (freq, weight, pos) in weights.items():\n",
    "      print(f\"Document: {doc_id}, Term: {term}, Frequency: {freq}, Weight: {weight} ,position:{pos}\")\n",
    "\n",
    "\n",
    "    text=\"\"\n",
    "    listw=[]\n",
    "    with open(f'./result_dir/description{j.__name__}.txt', 'w') as f:\n",
    "     for (doc_id, term), (freq, weight, pos) in weights.items():\n",
    "        f.write(f\"{doc_id} {term} {freq} {weight} {pos}\\n\")\n",
    "    with open(f'./result_dir/inv{j.__name__}.txt', 'w') as f:\n",
    "     for (doc_id, term), (freq, weight, pos) in weights.items():\n",
    "        f.write(f\" {term} {doc_id} {freq} {weight} {pos}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries saved in folder: test_dir\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# # Input and output directories\n",
    "# input_file = \"./med/MED.ALL\"  # Replace with the name of your input file\n",
    "# output_dir = \"test_dir\"\n",
    "\n",
    "# # Ensure the output directory exists\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Open the input file and process it\n",
    "# with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "#     content = file.read()\n",
    "\n",
    "# # Split the text by \".I \" which marks the start of a new entry\n",
    "# entries = content.split(\".I \")\n",
    "\n",
    "# # Process each entry\n",
    "# for entry in entries:\n",
    "#     if entry.strip():  # Skip empty sections\n",
    "#         # Extract the entry number and text\n",
    "#         lines = entry.split(\"\\n\", 2)  # Split into entry number, .W, and the main text\n",
    "#         entry_number = lines[0].strip()\n",
    "#         entry_text = lines[2] if len(lines) > 2 else \"\"  # Skip the .W line and take the rest\n",
    "        \n",
    "#         # Write each entry to its own file\n",
    "#         output_file = os.path.join(output_dir, f\"D_{entry_number}.txt\")\n",
    "#         with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
    "#             output.write(entry_text.strip())\n",
    "\n",
    "# print(f\"Entries saved in folder: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ouvrir le fichier et structurer les données\n",
    "data_dict = {}\n",
    "\n",
    "# Lire le fichier\n",
    "with open(\"./med/MED.REL\", \"r\") as file:\n",
    "    for line in file:\n",
    "        parts = line.split()\n",
    "        if len(parts) == 4:  # S'assurer que chaque ligne a 4 parties\n",
    "            key = int(parts[0])  # Premier numéro comme clé\n",
    "            value = int(parts[2])  # Troisième numéro\n",
    "            if key not in data_dict:\n",
    "                data_dict[key] = []  # Initialiser la liste si nécessaire\n",
    "            data_dict[key].append(value)\n",
    "\n",
    "# Convertir en liste de tuples\n",
    "result = [(key, values) for key, values in data_dict.items()]\n",
    "\n",
    "# Afficher le résultat\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
